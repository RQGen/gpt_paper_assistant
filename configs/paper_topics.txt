 1. New systems or methods regarding using LLMs to improve researchers' skill or process of research.
    - Relevant: HCI papers, especially system works, that discusses LLM application in research or science of science.
    - Not relevant: papers that are for general application.
 2. New methodological improvements to RLHF or instruction-following which are specific fine-tuning steps that are taken to make language models better at following user instructions across a range of tasks.
    - Relevant: papers that discuss specific methods like RLHF, or instruction-tuning datasets, improving these methods, or analyzing them. Usually these papers will explicitly mention RLHF, instruction-following or instruction-tuning.
    - Not relevant: papers about adaptation to some task. Simply following instructions or inputs are not sufficient.
 3. Shows new powerful test set contamination or membership inference methods for language models. Test set contamination is the phenomenon where a language model observes a benchmark dataset during pretraining.
    - Relevant: test statistics that can detect contamination of benchmarks in language models. statistics that can provide guarantees are more interesting. membership inference methods that are general enough to apply to language models are also relevant.
    - Not relevant: any papers that do not consider language models, or that do not consider test set contamination.
 4. Describes new paradigms to evaluating open-ended text generation. Evaluating the outputs of language models is hard, especially in open-ended settings like for chatbots.
    - Relevant: papers that fundamentally rethink language model evaluation -- especially by accounting for subjectivity or using adversaries.
    - Not relevant: specific evaluations for specific tasks, identifying new properties or flaws of language models, or simply collecting new data.
 5. Conducts surveys or provides data into real-world usage and safety properties of language models.
    - Relevant: papers that create new datasets or surveys on real-world usage of language models.
    - Not relevant: papers that apply language models to new real-world tasks.

 In suggesting papers to your friend, remember that he enjoys papers on human computer interaction and designs using LLMs, especially those related to "science of science".
 Your friend also likes learning about surprising empirical results in language models, as well as clever statistical tricks.